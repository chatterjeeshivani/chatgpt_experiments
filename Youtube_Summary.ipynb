{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOz6651LA7mEWDuUeIMuhxo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chatterjeeshivani/chatgpt_experiments/blob/main/Youtube_Summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "ooEIje9r1Knd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "727273da-5c5a-42d3-c6ef-116db27e664c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.2 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "JK20WGTSp_gt",
        "outputId": "5c426ec0-7b91-4bff-e4e3-014c3e3bdfb4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a18a8e9d-516b-4b11-bb5b-b08b71a0fea9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a18a8e9d-516b-4b11-bb5b-b08b71a0fea9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving CHATGPT INTRO - Silicon Dojo Seminar.txt to CHATGPT INTRO - Silicon Dojo Seminar.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WLC3uiIVoT9O"
      },
      "outputs": [],
      "source": [
        "with open('open_api_key.txt', 'r') as f:\n",
        "    openai_key = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('CHATGPT INTRO - Silicon Dojo Seminar.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "woTszoEYp4bN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_id=\"585DQv6nmlo\""
      ],
      "metadata": {
        "id": "ufBQwecQgIxC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRkY8q1gqKcy",
        "outputId": "c5ed4687-143d-435e-e24e-af54bc7cf757"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re"
      ],
      "metadata": {
        "id": "zCjxiXiGqWDc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Divide the transcript into segments without cutting words\n",
        "def split_into_segments(text, max_segment_length=2048):\n",
        "    words = text.split()\n",
        "    segments = []\n",
        "    current_segment = []\n",
        "\n",
        "    for word in words:\n",
        "        if sum(len(w) for w in current_segment) + len(word) + len(current_segment) > max_segment_length:\n",
        "            segments.append(\" \".join(current_segment))\n",
        "            current_segment = []\n",
        "\n",
        "        current_segment.append(word)\n",
        "\n",
        "    if current_segment:\n",
        "        segments.append(\" \".join(current_segment))\n",
        "\n",
        "    return segments\n",
        "\n"
      ],
      "metadata": {
        "id": "eXgt9ZIp1Abu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segments = split_into_segments(text)\n"
      ],
      "metadata": {
        "id": "GFjW_MUB1W_q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(segments))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwlHs4Dw1ZCz",
        "outputId": "129b5823-9322-4db3-eca9-b4a27efe2b75"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(segments)"
      ],
      "metadata": {
        "id": "TMlUM1Bb1epk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key=openai_key"
      ],
      "metadata": {
        "id": "YQSdCNSHAT0f"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(segment):\n",
        "    prompt=f\"Craft a concise, first-person blog paragraph focused solely on the technical aspects of the topic. Use the provided information to create a clear and engaging paragraph, incorporating only relevant technical details, important statistics, data points, or facts that support the main arguments. Make sure not to mention any specific speakers, podcasts, or transcription snippets in your response, and keep the content strictly technical and to-the-point:\\n\\n{segment}\\n\\nSummary:\"\n",
        "    response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "          {\"role\": \"user\", \"content\": prompt}\n",
        "      ]\n",
        "    )\n",
        "    summary = response.choices[0].message.content.strip()\n",
        "    print(summary)\n",
        "    return summary\n",
        "    #print(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "YsiwJ-mR1m12"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(segments[2])\n",
        "print(generate_summary(segments[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvfI0INLAKed",
        "outputId": "c44dcb7d-8cde-47a0-950b-f8875fb7051d"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you're gonna have to remove all those disclaimers and so it might be easier to use an older model so that's the thing like when the chat kpt 4.0 comes out when you're actually able to use it one it's actually going to cost more money to use because a more powerful model so it may not matter for whatever it is that you're trying to do and also it's manner of speaking Um might be such that you realize that it's actually easier to parse the responses from older models so that's that's just something to keep in mind with this particular seminar okay so I'm going to tell you a secret I'm going to tell you a secret and to be honest this might get my geek card taken away so so take this to heart one of the ways the technology professionals like me continue to get paid a lot of money many times for doing slightly stupid tasks is Because average people people users think what we do is much more complicated than it actually is and so they go I could never do that here take a lot of money and solve my problem right this is important we start talking about things like artificial intelligence because there's a lot of people out there and they're like Eli Eli I could never do artificial intelligence I'm not good at math I don't understand statistics you know all That fancy coding I just can't get it through my head Eli I'm Gonna Leave AI to other people well here's the thing I'm gonna tell you I'm going to tell you a secret I'm not actually that great with math I did take a statistics course in college I will say I passed it I think I passed it with a c but here's the deal it doesn't actually matter right we're not really dealing with AI we're not really dealing with artificial intelligence we're Dealing with an API apis are not AI apis give us access to Ai and what this means is basically we can write 10 lines of code in Python and get all the power of AI while having no clue how we actually get the response that's the amazing thing with the modern world it's something we call serverless architecture the modern architecture\n",
            "The upcoming Chat Kpt 4.0 release may not necessarily be the most cost-effective option for users due to its powerful nature, which could become more expensive to use. Furthermore, parsing responses from older models may prove to be easier than those from newer, more advanced models. While artificial intelligence may be perceived as a complex task, the truth is that APIs provide access to AI, allowing for the power of AI with the simplicity of 10 lines of Python code. This is made possible through serverless architecture, which is a modern architecture that removes the need for specific math or statistics capabilities.\n",
            "The upcoming Chat Kpt 4.0 release may not necessarily be the most cost-effective option for users due to its powerful nature, which could become more expensive to use. Furthermore, parsing responses from older models may prove to be easier than those from newer, more advanced models. While artificial intelligence may be perceived as a complex task, the truth is that APIs provide access to AI, allowing for the power of AI with the simplicity of 10 lines of Python code. This is made possible through serverless architecture, which is a modern architecture that removes the need for specific math or statistics capabilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#segments[0]"
      ],
      "metadata": {
        "id": "plgcbrs7aaxe"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = [generate_summary(segment) for segment in segments]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI9RehuZAPTz",
        "outputId": "443a8f1e-1590-45e6-ee03-55022d0ba0bc"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silicon Dojo is a technology education program in Asheville, North Carolina that offers Hands-On training to empower students. They rely on donations to support their classroom costs and equipment. In a recent seminar, they introduced the Chat GPT API and demonstrated how to use the DaVinci model, 3.5 model, Dolly image creation model, and Whisper audio tool. While the Chat GPT API version 4 exists, it is only available to large enterprises, and most people are on a waitlist.\n",
            "In this seminar, I will show you why it is important to watch even if you think the older models are obsolete. The DaVinci and 3.5 models provide different answers, and each model is good at doing specific things. When working with an API, it is crucial to understand that your program will send a request to Chatpt, and the AI will send back a response that you have to parse and format into something that is relevant to the end-user. The 3.5 turbo model can be verbose and sometimes irrelevant due to its politically correct language, making it challenging to parse. In contrast, the 3.0 model does not contain this AI language and can provide a cleaner response for the end-user. As a coder, understanding the different technical aspects of each model will help you create a project tailored to your needs.\n",
            "The upcoming chatbot version, Chat KPT 4.0, will cost more than older models due to increased power requirements. In addition, the language used may be easier to understand in older models, so it's important to keep this potential issue in mind. Many users think that employing AI is complicated, but in reality, APIs give us access to AI, allowing us to write just a few lines of code in Python to access powerful AI capabilities without needing to understand its inner workings. This is due to the modern serverless architecture.\n",
            "The technical aspects of using an AI system involve writing a few lines of Python code and sending a query up to a server. The open AI infrastructure then generates a text response, which is easily parsed as ASCII text. Although artificial intelligence is a complex field of study, using AI through APIs is relatively simple and typically requires only four to ten lines of code. However, technology professionals should not neglect the importance of understanding laws and regulations, maintenance contracts, and licensing schemes, as these are also essential aspects of their job.\n",
            "In the U.S. legal system, copyright protection is granted to human creators. If an AI creates something, there is no copyright protection for anyone. This was shown in the case of the famous \"monkey selfie\" photo, where the photographer who owned the camera and had set up everything prior to the monkey taking the photo was taken to court for not having a copyright. Since the monkey took the photo, and the AI does not have personhood, there is no one to claim copyright.\n",
            "A recent court case confirmed that scripts for creative works, such as a comic book or book, are copyrightable, but graphics created using AI graphics generators are not. This means that creators who use AI to generate graphics for their work will not own the copyright to those graphics, which can be problematic for merchandise and legal recourse against piracy. Using AI to create a look and feel for an app or startup company can also lead to copyright ownership issues. It's important to be aware of the limitations of AI-generated graphics when creating creative works.\n",
            "In the world of Chat GPT, creating original content is crucial as copycats can easily replicate and profit from your work without legal recourse. It may be smarter to hire a graphic designer and own the copyright. When dealing with Dolly or other AI-generated content, you have the right to use it for merchandise, but be aware that others can also profit from it. Tokens are the currency of Chat GPT, used to purchase AI-generated content. They are often represented by short character or word combinations, which can vary depending on how the AI breaks down the token count.\n",
            "When using the Chat GPT API, it's important to keep in mind that tokens are used for both queries and responses. The token cost for queries can be significant, especially with the Chat GPT4 model where you can input up to 16 pages of information into one query. While the pricing model can be wonky, with prices as tiny as 1 cent per thousand tokens, it's still important to be mindful of the token cost for queries and responses to ensure you don't go bankrupt while using the API. However, in the enterprise world, the cost for individual queries may be insignificant if you're churning through many queries.\n",
            "GPT-3's pricing system is based on tokens, where 1,000 tokens cost a fifth of a penny. The cost of each query or response may seem insignificant, but it can quickly add up, especially in an enterprise setting with a high volume of users. However, GPT-3 is still relatively inexpensive, with one user spending only $2 in testing and playing classes. The pricing model includes multiple models with different capabilities and price points, with the DaVinci and turbo models being the most cost-effective. On the other hand, the Dolly model's query to image output feature can be expensive, so users should be cautious when using it.\n",
            "When using chatbot Dolly to generate images, it's important to note that the different image sizes have varying costs. A 1024 image costs 2 cents per image, a 512 image costs 1.8 cents per image, and a 256 image costs 1.6 cents per image. While these prices may seem cheap, it's worth considering that Dolly often produces strange or unusable images. To counteract this, users can request multiple images to choose from. However, this can quickly become expensive, even at such low rates, as 10 1024 images cost 20 cents per run of the script. When using Chat GPT, the text generation feature should be inexpensive, but it's important to keep in mind the potential costs associated with image generation. To pay for these services, users can set up billing information through the account settings menu.\n",
            "When using chatbots connected to APIs, it's important to set a monthly cap to avoid unexpected costs from overuse. It's also crucial to choose the right model for your needs. While access to the newest model, pt4, isn't available yet, GPT 3.5 turbo and DaVinci zero zero three are both good options. The 3.5 model is optimized for chat and costs one-tenth the price of DaVinci. When selecting a model, it's important to consider when the training data was last updated to ensure accurate responses. The training data for these models goes up to September 2021, so any questions outside of that range may not be understood.\n",
            "DaVinci is a model that can perform any language task with better quality, longer output, and consistent instruction following than Curry Babbage and Ada models. The training data for the model is up to June of 2021. One important thing to keep in mind when switching models is the endpoint compatibility, as the way to input queries and parse responses may differ. DaVinci model is much less verbose than the GPT 3.5 turbo model, which can be seen in practical code examples.\n",
            "In order to secure your API key, it's suggested to use environment variables instead of hard-coding them in the code. This way, no one can simply copy or screen-capture your API key. In Python, you'll need to import the module you're going to use and install it with pip. For the open AI module, you will then feed it your API key, and create a question/query. The chat EPT 3.5 turbo open AI chat completion create function is used to send the question to the model, and the response is printed out.\n",
            "When using the OpenAI completion model, one must provide a prompt and adjust the temperature to define the quality of the answer. Another important parameter is max tokens, which determines the maximum number of tokens to use for the response. It is key to provide enough tokens, otherwise the model can fail in the middle of a sentence, costing the user more tokens. The frequency penalty and presence penalty can be adjusted later in the process. For the DaVinci model, a different format is used to print out the response, which is provided as choices with a zero index of text. These technical details are important to keep in mind when using these models.\n",
            "As a technology professional, it's important to consider the speed of different AI models when designing programs. Turbo 3.5 and DaVinci are two models used to answer subjective questions, such as the existence of God. However, as an AI language model, these models may provide unnecessary or unhelpful responses, and it's important to know which model to use to get the concise and clear answer that users want. Consider using the older, more concise model when appropriate, rather than assuming the newest model is always the best.\n",
            "When dealing with end points in Python, it's important to keep in mind the differences between dictionaries and lists. Dictionaries are named key arrays, while lists are standard arrays. To print out the text in a response from Chat TPT 3.5 or Chat GPT 3.0, you'll need to know how to access the relevant information. For example, in Chat GPT 3.5, you would access the response by calling \"response choices at Index 0 message content\". Knowing these technical details is essential for successful coding with these models.\n",
            "When using the GPT-3 3.0 DaVinci model, it's important to understand how lists and dictionaries work to access the values you need. The response includes a list with various choices, each with its own index starting at 0, and the choice's text is what you're looking for. If you're coding for something more complicated, you'll need to go through this process to figure out where the value you're looking for is located. It's not too complicated, but it's essential to understand when printing out the desired text from the endpoint. In the next section, we'll demonstrate how the DaVinci model responds to different prompts.\n",
            "In order to demonstrate the capabilities of DaVinci, we will have it write a blog post in HTML format and communicate with an employee. To begin, we import the open AI module and add the API key. We then use the open AI completion create function to send our prompt to chat GPT and receive a large endpoint response. We'll use the DaVinci 003 model, with a temperature and a maximum of 1000 tokens. We'll print out the entire response and then extract the text. For our prompt, we'll ask for a story about a frog and a unicycle. Finally, we save the code and run it to see the response.\n",
            "This paragraph discusses the technical aspects of using OpenAI's DaVinci API to generate a story about a frog and a unicycle. The author highlights the use of Python 3 and the number of tokens used to complete the task, as well as the text generated by index 0. The generated story is displayed and the author describes the quality of the writing as impressive, suggesting that it could replace traditional bookstores for generating bedtime stories. The paragraph concludes by emphasizing the significance of generating stories through code.\n",
            "As a developer dealing with multiple technologies and coding languages, sometimes all I need is a solution to a specific problem, such as turning a Django query set into a dictionary. With Python's Django web app framework, query sets retrieved from the database can be transformed into dictionaries using the dot values method. While Python is used for back-end development, front-end development may require additional languages like JavaScript. Asking open AI can provide quick and targeted technical responses, without the overwhelming amount of results often found with general search engines like Google.\n",
            "The technology being discussed allows for quick and accurate answers to questions that can be repeated if necessary. Furthermore, the technology can also format the text in HTML, which is useful for creating blog posts or articles. This function ensures that content is correctly formatted and ready for use on a website. The formatted text is also ready for submission into a database.\n",
            "Automating web pages is a cool task that can be accomplished easily by calling them from WordPress or similar platforms. Salesforce intends to replace cold calling with AI-driven chatbots that create cold emails tailored to clients. There is a risk that AI will increase the amount of spam that users receive every day. OpenAI's DaVinci 3.0 model can be used to automate several tasks, including creating personalized emails to potential buyers.\n",
            "The 3.5 turbo model uses a different coating than the DaVinci model and has five different lines of code. One interesting feature is the ability to assign roles to nudge the model in a particular direction. Reusability is important in creating technological products, and the turbo model allows for writing a question and nudging it into a specific direction. This is useful when the answer depends on the user's location. One can import the open AI module, use the API key, and create a variable value to tweak the answer received. The response is equal to the open AI dot cat completion create function.\n",
            "The technical aspects of Chat GPT 3.5 involve three roles: system, assistant, and user. The system role determines the character profile of the AI, such as the role of an advisor or the president of a country. The assistant role helps nudge the AI in a particular direction, such as answering as a USA citizen. The user role is the actual question being asked. By using the assistant role, the AI can cater its answer to a particular demographic, such as a woman living in California.\n",
            "In this technical discussion, we explore a web application that uses the OpenAI Turbo programming language to provide customized responses to user queries. The application receives input from users with distinct demographic information, such as their age and gender, and uses this information to bias response outputs. For instance, if the user is above 55 years old, the application biases output based on that information. Furthermore, the output response may include additional information related to the query. However, the OpenAI Turbo may be a bit verbose, so developers should consider this when parsing responses. Additionally, response times may vary depending on the input, and developers need to take this into account when creating the web application.\n",
            "AI technology allows for customized answers to questions based on user demographics and specific information. The use of assistants and variables can help skew answers in a particular direction to ensure the user gets the most appropriate response. Concatenating additional information to the query can also influence the direction of the answer. One of the powerful aspects of AI is the psychological impact that people believe computers are always right, which can be leveraged to provide a more trustworthy response.\n",
            "In the world of information technology, it's important to remember that tasks are based on the human condition. Technology is used to solve human problems in ways that humans want them to be solved. When writing code, it's crucial to think beyond loops and variable values and focus on how the app will be used. Adding a little bit of humanity into the app is necessary. In this example, a specific app utilizes a Da Vinci model and the OpenAI module with an API key, allowing for the concatenation of additional information in a particular script. Based on the type of person asking the question, the app will give an entirely different response, either making things fun or dark. There's a common misconception that technology is pure and humans are scary, but technology exists to solve human problems in human ways.\n",
            "In the Slant list, we have a diverse group of members including Christian, scientists, pastafarians, Republicans, and Democrats. It's interesting to note the bias of chatbot GPT towards political affiliations, particularly towards Republicans or Democrats. Using a for each loop, we can submit a question to the model asking how each group of members thinks about the beginning of the world. With a temperature set, we print out the string of the group and the answer to the question. This allows us to see any potential biases within the GPT model.\n",
            "As a Christian, I believe that God created the world in six days according to the Bible. However, as a scientist, the prevailing belief is that the world began with the Big Bang Theory. Interestingly, Pastafarians say the Flying Spaghetti Monster created the world after drinking heavily. Republicans generally believe that God created the universe and all its contents, while Democrats believe that the world began with the Big Bang Theory. Concatenating different biases can lead to entirely different answers, so it's essential to consider this when developing AI apps.\n",
            "The technical aspect of the topic involves automating the process of formatting blog posts in HTML format by allowing users to simply type in the prompt or question. This can prove to be helpful as the rules of English language differ from region to region. By adding concatenation directly into the prompt, users can ensure that the syntax and language used is appropriate for the target audience. Additionally, Dolly, while an amazing technological advancement, can be quite creepy as it generates images based on given prompts.\n",
            "When using Dolly, it's important to remember that the URL provided will only be active for a limited amount of time, possibly around an hour. Therefore, it's recommended to download any images embedded into websites using something like wget to prevent the image from disappearing. The output from Dolly can vary greatly depending on the prompt, with normal prompts sometimes leading to surprisingly dark results. Crazy prompts will often yield equally bizarre outputs.\n",
            "The code for generating AI-generated images involves importing the OpenAI API, creating a prompt and giving it a number, size, and image URL. The response will display the generated images' URLs, which can be embedded onto a document. The size of the image can be adjusted depending on the user's preference, and the cost of generating an image will depend on the size and number of images requested. The code provided in the blog is a technical demonstration of how image creation through AI can be achieved.\n",
            "In this technical blog post, I will demonstrate how to print to a Dolly test file and embed images using the IMG SRC to display everything on one page. We will not be downloading the images automatically, but getting the URLs for them. By clicking on the URLs, we can open the images. The script will run again and again every time we refresh the page, displaying a new set of images along with any changes we have made.\n",
            "The blog post discusses the technical aspects of both the image API with DALL-E and audio API with Whisper. The Whisper API is currently in beta, so users don't need to give any tokens when uploading content to get a transcript. The code for transcribing audio is very simple and only requires three lines of code with the option to translate audio into 50 languages. However, it's not currently possible to translate languages from English into another language. The blog post also provides an API key for using the open AI platform.\n",
            "In this technical blog post, we will learn how to use the OpenAI Audio Transcribe Whisper API to transcribe an audio file into text. The first step is to define the variable for the audio file and open it in Python. Then, we will use the Audio Transcribe Whisper API to transcribe the audio using the whisper one model. Once the transcript is generated, we will print it out. To test this out, we can use Audacity to record an audio file and export it as an MP3 file. Then, we can use our Python script to transcribe the audio file into text. While dealing with audio files can be challenging, turning them into text files is simple. Finally, we will discuss an example of how JavaScript can be used to record audio on a web page.\n",
            "Using the web app, audio is turned into MP3 and submitted to Whisper. Whisper then sends back the text from the audio file, which can be parsed for a command. This allows the user to use Chad GPT or any other query to get the desired results. China GPT API also offers moderation, which can scan communications and look for hatefulness or other red flags. This can be useful for organizations looking to manage their employees' or users' messages for red flags. Overall, Whisper and China GPT API offer efficient ways to convert audio to text and moderate communications.\n",
            "As a manager, it can be difficult to keep track of all employees and identify those who may be struggling or becoming frustrated. This tunnel vision can lead to neglecting middle performers, who are valuable to the company but not necessarily the best or worst. AI systems with a moderation type system can provide a single pane of glass to analyze the overall institution, identify potential issues, and prevent an explosion in the middle group of employees. By constantly scanning messages and establishing baseline behaviors, managers can address issues before they become problematic.\n",
            "To monitor employee communication in order to prevent potential issues such as bullying, hate speech, or sexual harassment, implementing a moderation system can be very effective. By utilizing the Point value system to observe spikes in negative communication, managers can intervene early and address the problem before it escalates into lawsuits. With the help of Python and the OpenAI module, this moderation system can be easily implemented by importing the necessary modules and providing the input source, which can be from a database or any other source. It is important to note that bullying and discrimination do not stop in adulthood, making it essential to monitor and address any negative communication among employees.\n",
            "The moderation API has different categories for identifying hate speech, self-harm, sexual content, and violent content. Each category is marked with a true or false based on the input from the text. The moderation API also provides an actual number score that identifies the severity of the content. These scores can be helpful in identifying the urgency of the situation and allowing for quick action to be taken. In a business environment, it is essential to nip any inappropriate communication in the bud to avoid negative consequences for the company.\n",
            "This paragraph discusses the technical aspects of using an API to monitor and analyze user communication. The API can analyze the frequency and severity of various types of communication, such as hate speech, threats, self-harm, and violence. By tracking this data, a company can establish a baseline for their employees' communication and detect deviations or spikes. This is crucial for management to understand how the organization communicates and identify any potential legal issues. The API can be used to scan various communication channels, including messaging systems, email, and social media. It can also help identify user abuse by detecting patterns of problematic communication.\n",
            "To prevent API violations and maintain access to APIs from companies, it's important to track individual user activity. You can add a user variable that pulls values from a database or a session so that when the account is flagged for an API violation, you know who is responsible. Without a way to identify individual violators, access to API may be cut off. Additionally, if using Open AI solutions, caching results can help save money as every image requested comes at a cost.\n",
            "Automatic image downloads into a data store can save users time and possibly provide them with images they didn't know they needed. Caching GPT results can also be beneficial by saving commonly asked questions and answers in a database that can be edited for appropriateness. This can prevent redundant API use and save on fees while also allowing for tailored responses to be given to users. It's important to recognize that humans generally have similar questions and problems, making caching a valuable tool for businesses and organizations.\n",
            "When dealing with a large number of users on a web app, reducing API usage fees and improving operational security through caching becomes crucial. Caching allows for local infrastructure use, preventing man-in-the-middle attacks and other security risks. It is important to note that as of March 2023, chat EPT is not using questions to teach their models. However, it is essential to consider the potential impact of self-learning models as seen with Amazon, where engineers copied proprietary code into chat EPT, resulting in the AI solution learning and potentially revealing costly secrets.\n",
            "When working with APIs, it's important to be aware of potential risks, such as not knowing how vendors are handling queries or logging information. It's crucial to only send necessary and sanitized information to avoid leaks. Additionally, when creating apps to submit to AI solutions, ensure there is sanitization within the app to remove API keys. It's important to build for a zero trust environment to avoid potential competitors or changes in learning from input information. GPT4 is an upcoming feature, and waitlists have opened for access, but it's important to also consider other models, such as Whisper, Dolly, and DaVinci.\n",
            "To remain relevant in the future, it's important to understand that even new models may not solve your problem better than previous versions. Learning Python is important for those who want to start playing with AI APIs such as Open AI, which seems to be a \"Python first environment.\" There is a Node JS API available, but it may not be at the same level as the Python API. If you know a different coding language, there may not be official APIs available, but workarounds can be found on GitHub. To start playing with GPT, it's not necessary to know much Python, and the chat GPT API initially offers 18 in credit (or possibly $5).\n",
            "If you want to start understanding AI and machine learning, sign up for the chat GPT API, which provides credit to use with text. However, using images may cost a decent amount of money. To continue using the API, simply input your credit card information. The concept of scaling silicon Dojo is to create your own martial arts studio, with no copyright on the different techniques taught. Many people have benefitted from these tutorials, with one individual from Africa downloading videos to bring back to their village without internet access.\n",
            "Silicon Dojo's founder discusses how his methods of teaching coding and programming skills can be replicated globally through affordable means. He believes that his tutorials can be followed in any location, as they only require a 700 square foot space to train others in Python and Linux programming languages. He encourages many people who have engaged with Silicon Dojo to adopt his methods in their hometowns, citing how it doesn't take many resources or financial input to make a difference. The tutorial code is open-source and available for download on GitHub to modify and use at one's discretion.\n",
            "Silicon Dojo will be focusing more on Hands-On classes to keep students engaged and awake. The plan is to cut down the three eight-hour Hands-On classes into one to three hour blocks and introduce Python as a basic concept. The focus will be on explaining modules, variables, if else statements, loops, and other important concepts. The goal is to make students comfortable with the programming language and give them a tactile experience of building something. This approach is expected to improve student engagement and help them retain more information.\n",
            "Python is the de facto language for a versatile education curriculum, covering topics such as robotics, AI, and computer vision. A proposed method involves creating 50 blocks of classes on Python, covering areas like databases, chat EPT, OpenCV, and Azure cognitive services. These blocks can be mixed and matched to cater to specific needs, such as a full-day class or boot camp. The approach mirrors a business-like method, finding a replicatable product through these Hands-On classes. Whether it will be successful, only time will tell. Overall, empowering students to create a better world is the goal of these classes, and the instructor hopes to see students attend in-person or watch on YouTube.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#summaries"
      ],
      "metadata": {
        "id": "GayYP6coDEuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Combine the summaries to create a step-by-step summary of the podcast\n",
        "\n",
        "final_summary = \"\\n\".join(summary for summary in summaries if summary is not None)\n",
        "print(\"Step-by-step summary of the podcast:\")\n",
        "print(final_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2F56lnfA1Qx",
        "outputId": "29487bac-e374-41f0-d0e1-544ac36c074e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-by-step summary of the podcast:\n",
            "Silicon Dojo is a technology education program in Asheville, North Carolina that offers Hands-On training to empower students. They rely on donations to support their classroom costs and equipment. In a recent seminar, they introduced the Chat GPT API and demonstrated how to use the DaVinci model, 3.5 model, Dolly image creation model, and Whisper audio tool. While the Chat GPT API version 4 exists, it is only available to large enterprises, and most people are on a waitlist.\n",
            "In this seminar, I will show you why it is important to watch even if you think the older models are obsolete. The DaVinci and 3.5 models provide different answers, and each model is good at doing specific things. When working with an API, it is crucial to understand that your program will send a request to Chatpt, and the AI will send back a response that you have to parse and format into something that is relevant to the end-user. The 3.5 turbo model can be verbose and sometimes irrelevant due to its politically correct language, making it challenging to parse. In contrast, the 3.0 model does not contain this AI language and can provide a cleaner response for the end-user. As a coder, understanding the different technical aspects of each model will help you create a project tailored to your needs.\n",
            "The upcoming chatbot version, Chat KPT 4.0, will cost more than older models due to increased power requirements. In addition, the language used may be easier to understand in older models, so it's important to keep this potential issue in mind. Many users think that employing AI is complicated, but in reality, APIs give us access to AI, allowing us to write just a few lines of code in Python to access powerful AI capabilities without needing to understand its inner workings. This is due to the modern serverless architecture.\n",
            "The technical aspects of using an AI system involve writing a few lines of Python code and sending a query up to a server. The open AI infrastructure then generates a text response, which is easily parsed as ASCII text. Although artificial intelligence is a complex field of study, using AI through APIs is relatively simple and typically requires only four to ten lines of code. However, technology professionals should not neglect the importance of understanding laws and regulations, maintenance contracts, and licensing schemes, as these are also essential aspects of their job.\n",
            "In the U.S. legal system, copyright protection is granted to human creators. If an AI creates something, there is no copyright protection for anyone. This was shown in the case of the famous \"monkey selfie\" photo, where the photographer who owned the camera and had set up everything prior to the monkey taking the photo was taken to court for not having a copyright. Since the monkey took the photo, and the AI does not have personhood, there is no one to claim copyright.\n",
            "A recent court case confirmed that scripts for creative works, such as a comic book or book, are copyrightable, but graphics created using AI graphics generators are not. This means that creators who use AI to generate graphics for their work will not own the copyright to those graphics, which can be problematic for merchandise and legal recourse against piracy. Using AI to create a look and feel for an app or startup company can also lead to copyright ownership issues. It's important to be aware of the limitations of AI-generated graphics when creating creative works.\n",
            "In the world of Chat GPT, creating original content is crucial as copycats can easily replicate and profit from your work without legal recourse. It may be smarter to hire a graphic designer and own the copyright. When dealing with Dolly or other AI-generated content, you have the right to use it for merchandise, but be aware that others can also profit from it. Tokens are the currency of Chat GPT, used to purchase AI-generated content. They are often represented by short character or word combinations, which can vary depending on how the AI breaks down the token count.\n",
            "When using the Chat GPT API, it's important to keep in mind that tokens are used for both queries and responses. The token cost for queries can be significant, especially with the Chat GPT4 model where you can input up to 16 pages of information into one query. While the pricing model can be wonky, with prices as tiny as 1 cent per thousand tokens, it's still important to be mindful of the token cost for queries and responses to ensure you don't go bankrupt while using the API. However, in the enterprise world, the cost for individual queries may be insignificant if you're churning through many queries.\n",
            "GPT-3's pricing system is based on tokens, where 1,000 tokens cost a fifth of a penny. The cost of each query or response may seem insignificant, but it can quickly add up, especially in an enterprise setting with a high volume of users. However, GPT-3 is still relatively inexpensive, with one user spending only $2 in testing and playing classes. The pricing model includes multiple models with different capabilities and price points, with the DaVinci and turbo models being the most cost-effective. On the other hand, the Dolly model's query to image output feature can be expensive, so users should be cautious when using it.\n",
            "When using chatbot Dolly to generate images, it's important to note that the different image sizes have varying costs. A 1024 image costs 2 cents per image, a 512 image costs 1.8 cents per image, and a 256 image costs 1.6 cents per image. While these prices may seem cheap, it's worth considering that Dolly often produces strange or unusable images. To counteract this, users can request multiple images to choose from. However, this can quickly become expensive, even at such low rates, as 10 1024 images cost 20 cents per run of the script. When using Chat GPT, the text generation feature should be inexpensive, but it's important to keep in mind the potential costs associated with image generation. To pay for these services, users can set up billing information through the account settings menu.\n",
            "When using chatbots connected to APIs, it's important to set a monthly cap to avoid unexpected costs from overuse. It's also crucial to choose the right model for your needs. While access to the newest model, pt4, isn't available yet, GPT 3.5 turbo and DaVinci zero zero three are both good options. The 3.5 model is optimized for chat and costs one-tenth the price of DaVinci. When selecting a model, it's important to consider when the training data was last updated to ensure accurate responses. The training data for these models goes up to September 2021, so any questions outside of that range may not be understood.\n",
            "DaVinci is a model that can perform any language task with better quality, longer output, and consistent instruction following than Curry Babbage and Ada models. The training data for the model is up to June of 2021. One important thing to keep in mind when switching models is the endpoint compatibility, as the way to input queries and parse responses may differ. DaVinci model is much less verbose than the GPT 3.5 turbo model, which can be seen in practical code examples.\n",
            "In order to secure your API key, it's suggested to use environment variables instead of hard-coding them in the code. This way, no one can simply copy or screen-capture your API key. In Python, you'll need to import the module you're going to use and install it with pip. For the open AI module, you will then feed it your API key, and create a question/query. The chat EPT 3.5 turbo open AI chat completion create function is used to send the question to the model, and the response is printed out.\n",
            "When using the OpenAI completion model, one must provide a prompt and adjust the temperature to define the quality of the answer. Another important parameter is max tokens, which determines the maximum number of tokens to use for the response. It is key to provide enough tokens, otherwise the model can fail in the middle of a sentence, costing the user more tokens. The frequency penalty and presence penalty can be adjusted later in the process. For the DaVinci model, a different format is used to print out the response, which is provided as choices with a zero index of text. These technical details are important to keep in mind when using these models.\n",
            "As a technology professional, it's important to consider the speed of different AI models when designing programs. Turbo 3.5 and DaVinci are two models used to answer subjective questions, such as the existence of God. However, as an AI language model, these models may provide unnecessary or unhelpful responses, and it's important to know which model to use to get the concise and clear answer that users want. Consider using the older, more concise model when appropriate, rather than assuming the newest model is always the best.\n",
            "When dealing with end points in Python, it's important to keep in mind the differences between dictionaries and lists. Dictionaries are named key arrays, while lists are standard arrays. To print out the text in a response from Chat TPT 3.5 or Chat GPT 3.0, you'll need to know how to access the relevant information. For example, in Chat GPT 3.5, you would access the response by calling \"response choices at Index 0 message content\". Knowing these technical details is essential for successful coding with these models.\n",
            "When using the GPT-3 3.0 DaVinci model, it's important to understand how lists and dictionaries work to access the values you need. The response includes a list with various choices, each with its own index starting at 0, and the choice's text is what you're looking for. If you're coding for something more complicated, you'll need to go through this process to figure out where the value you're looking for is located. It's not too complicated, but it's essential to understand when printing out the desired text from the endpoint. In the next section, we'll demonstrate how the DaVinci model responds to different prompts.\n",
            "In order to demonstrate the capabilities of DaVinci, we will have it write a blog post in HTML format and communicate with an employee. To begin, we import the open AI module and add the API key. We then use the open AI completion create function to send our prompt to chat GPT and receive a large endpoint response. We'll use the DaVinci 003 model, with a temperature and a maximum of 1000 tokens. We'll print out the entire response and then extract the text. For our prompt, we'll ask for a story about a frog and a unicycle. Finally, we save the code and run it to see the response.\n",
            "This paragraph discusses the technical aspects of using OpenAI's DaVinci API to generate a story about a frog and a unicycle. The author highlights the use of Python 3 and the number of tokens used to complete the task, as well as the text generated by index 0. The generated story is displayed and the author describes the quality of the writing as impressive, suggesting that it could replace traditional bookstores for generating bedtime stories. The paragraph concludes by emphasizing the significance of generating stories through code.\n",
            "As a developer dealing with multiple technologies and coding languages, sometimes all I need is a solution to a specific problem, such as turning a Django query set into a dictionary. With Python's Django web app framework, query sets retrieved from the database can be transformed into dictionaries using the dot values method. While Python is used for back-end development, front-end development may require additional languages like JavaScript. Asking open AI can provide quick and targeted technical responses, without the overwhelming amount of results often found with general search engines like Google.\n",
            "The technology being discussed allows for quick and accurate answers to questions that can be repeated if necessary. Furthermore, the technology can also format the text in HTML, which is useful for creating blog posts or articles. This function ensures that content is correctly formatted and ready for use on a website. The formatted text is also ready for submission into a database.\n",
            "Automating web pages is a cool task that can be accomplished easily by calling them from WordPress or similar platforms. Salesforce intends to replace cold calling with AI-driven chatbots that create cold emails tailored to clients. There is a risk that AI will increase the amount of spam that users receive every day. OpenAI's DaVinci 3.0 model can be used to automate several tasks, including creating personalized emails to potential buyers.\n",
            "The 3.5 turbo model uses a different coating than the DaVinci model and has five different lines of code. One interesting feature is the ability to assign roles to nudge the model in a particular direction. Reusability is important in creating technological products, and the turbo model allows for writing a question and nudging it into a specific direction. This is useful when the answer depends on the user's location. One can import the open AI module, use the API key, and create a variable value to tweak the answer received. The response is equal to the open AI dot cat completion create function.\n",
            "The technical aspects of Chat GPT 3.5 involve three roles: system, assistant, and user. The system role determines the character profile of the AI, such as the role of an advisor or the president of a country. The assistant role helps nudge the AI in a particular direction, such as answering as a USA citizen. The user role is the actual question being asked. By using the assistant role, the AI can cater its answer to a particular demographic, such as a woman living in California.\n",
            "In this technical discussion, we explore a web application that uses the OpenAI Turbo programming language to provide customized responses to user queries. The application receives input from users with distinct demographic information, such as their age and gender, and uses this information to bias response outputs. For instance, if the user is above 55 years old, the application biases output based on that information. Furthermore, the output response may include additional information related to the query. However, the OpenAI Turbo may be a bit verbose, so developers should consider this when parsing responses. Additionally, response times may vary depending on the input, and developers need to take this into account when creating the web application.\n",
            "AI technology allows for customized answers to questions based on user demographics and specific information. The use of assistants and variables can help skew answers in a particular direction to ensure the user gets the most appropriate response. Concatenating additional information to the query can also influence the direction of the answer. One of the powerful aspects of AI is the psychological impact that people believe computers are always right, which can be leveraged to provide a more trustworthy response.\n",
            "In the world of information technology, it's important to remember that tasks are based on the human condition. Technology is used to solve human problems in ways that humans want them to be solved. When writing code, it's crucial to think beyond loops and variable values and focus on how the app will be used. Adding a little bit of humanity into the app is necessary. In this example, a specific app utilizes a Da Vinci model and the OpenAI module with an API key, allowing for the concatenation of additional information in a particular script. Based on the type of person asking the question, the app will give an entirely different response, either making things fun or dark. There's a common misconception that technology is pure and humans are scary, but technology exists to solve human problems in human ways.\n",
            "In the Slant list, we have a diverse group of members including Christian, scientists, pastafarians, Republicans, and Democrats. It's interesting to note the bias of chatbot GPT towards political affiliations, particularly towards Republicans or Democrats. Using a for each loop, we can submit a question to the model asking how each group of members thinks about the beginning of the world. With a temperature set, we print out the string of the group and the answer to the question. This allows us to see any potential biases within the GPT model.\n",
            "As a Christian, I believe that God created the world in six days according to the Bible. However, as a scientist, the prevailing belief is that the world began with the Big Bang Theory. Interestingly, Pastafarians say the Flying Spaghetti Monster created the world after drinking heavily. Republicans generally believe that God created the universe and all its contents, while Democrats believe that the world began with the Big Bang Theory. Concatenating different biases can lead to entirely different answers, so it's essential to consider this when developing AI apps.\n",
            "The technical aspect of the topic involves automating the process of formatting blog posts in HTML format by allowing users to simply type in the prompt or question. This can prove to be helpful as the rules of English language differ from region to region. By adding concatenation directly into the prompt, users can ensure that the syntax and language used is appropriate for the target audience. Additionally, Dolly, while an amazing technological advancement, can be quite creepy as it generates images based on given prompts.\n",
            "When using Dolly, it's important to remember that the URL provided will only be active for a limited amount of time, possibly around an hour. Therefore, it's recommended to download any images embedded into websites using something like wget to prevent the image from disappearing. The output from Dolly can vary greatly depending on the prompt, with normal prompts sometimes leading to surprisingly dark results. Crazy prompts will often yield equally bizarre outputs.\n",
            "The code for generating AI-generated images involves importing the OpenAI API, creating a prompt and giving it a number, size, and image URL. The response will display the generated images' URLs, which can be embedded onto a document. The size of the image can be adjusted depending on the user's preference, and the cost of generating an image will depend on the size and number of images requested. The code provided in the blog is a technical demonstration of how image creation through AI can be achieved.\n",
            "In this technical blog post, I will demonstrate how to print to a Dolly test file and embed images using the IMG SRC to display everything on one page. We will not be downloading the images automatically, but getting the URLs for them. By clicking on the URLs, we can open the images. The script will run again and again every time we refresh the page, displaying a new set of images along with any changes we have made.\n",
            "The blog post discusses the technical aspects of both the image API with DALL-E and audio API with Whisper. The Whisper API is currently in beta, so users don't need to give any tokens when uploading content to get a transcript. The code for transcribing audio is very simple and only requires three lines of code with the option to translate audio into 50 languages. However, it's not currently possible to translate languages from English into another language. The blog post also provides an API key for using the open AI platform.\n",
            "In this technical blog post, we will learn how to use the OpenAI Audio Transcribe Whisper API to transcribe an audio file into text. The first step is to define the variable for the audio file and open it in Python. Then, we will use the Audio Transcribe Whisper API to transcribe the audio using the whisper one model. Once the transcript is generated, we will print it out. To test this out, we can use Audacity to record an audio file and export it as an MP3 file. Then, we can use our Python script to transcribe the audio file into text. While dealing with audio files can be challenging, turning them into text files is simple. Finally, we will discuss an example of how JavaScript can be used to record audio on a web page.\n",
            "Using the web app, audio is turned into MP3 and submitted to Whisper. Whisper then sends back the text from the audio file, which can be parsed for a command. This allows the user to use Chad GPT or any other query to get the desired results. China GPT API also offers moderation, which can scan communications and look for hatefulness or other red flags. This can be useful for organizations looking to manage their employees' or users' messages for red flags. Overall, Whisper and China GPT API offer efficient ways to convert audio to text and moderate communications.\n",
            "As a manager, it can be difficult to keep track of all employees and identify those who may be struggling or becoming frustrated. This tunnel vision can lead to neglecting middle performers, who are valuable to the company but not necessarily the best or worst. AI systems with a moderation type system can provide a single pane of glass to analyze the overall institution, identify potential issues, and prevent an explosion in the middle group of employees. By constantly scanning messages and establishing baseline behaviors, managers can address issues before they become problematic.\n",
            "To monitor employee communication in order to prevent potential issues such as bullying, hate speech, or sexual harassment, implementing a moderation system can be very effective. By utilizing the Point value system to observe spikes in negative communication, managers can intervene early and address the problem before it escalates into lawsuits. With the help of Python and the OpenAI module, this moderation system can be easily implemented by importing the necessary modules and providing the input source, which can be from a database or any other source. It is important to note that bullying and discrimination do not stop in adulthood, making it essential to monitor and address any negative communication among employees.\n",
            "The moderation API has different categories for identifying hate speech, self-harm, sexual content, and violent content. Each category is marked with a true or false based on the input from the text. The moderation API also provides an actual number score that identifies the severity of the content. These scores can be helpful in identifying the urgency of the situation and allowing for quick action to be taken. In a business environment, it is essential to nip any inappropriate communication in the bud to avoid negative consequences for the company.\n",
            "This paragraph discusses the technical aspects of using an API to monitor and analyze user communication. The API can analyze the frequency and severity of various types of communication, such as hate speech, threats, self-harm, and violence. By tracking this data, a company can establish a baseline for their employees' communication and detect deviations or spikes. This is crucial for management to understand how the organization communicates and identify any potential legal issues. The API can be used to scan various communication channels, including messaging systems, email, and social media. It can also help identify user abuse by detecting patterns of problematic communication.\n",
            "To prevent API violations and maintain access to APIs from companies, it's important to track individual user activity. You can add a user variable that pulls values from a database or a session so that when the account is flagged for an API violation, you know who is responsible. Without a way to identify individual violators, access to API may be cut off. Additionally, if using Open AI solutions, caching results can help save money as every image requested comes at a cost.\n",
            "Automatic image downloads into a data store can save users time and possibly provide them with images they didn't know they needed. Caching GPT results can also be beneficial by saving commonly asked questions and answers in a database that can be edited for appropriateness. This can prevent redundant API use and save on fees while also allowing for tailored responses to be given to users. It's important to recognize that humans generally have similar questions and problems, making caching a valuable tool for businesses and organizations.\n",
            "When dealing with a large number of users on a web app, reducing API usage fees and improving operational security through caching becomes crucial. Caching allows for local infrastructure use, preventing man-in-the-middle attacks and other security risks. It is important to note that as of March 2023, chat EPT is not using questions to teach their models. However, it is essential to consider the potential impact of self-learning models as seen with Amazon, where engineers copied proprietary code into chat EPT, resulting in the AI solution learning and potentially revealing costly secrets.\n",
            "When working with APIs, it's important to be aware of potential risks, such as not knowing how vendors are handling queries or logging information. It's crucial to only send necessary and sanitized information to avoid leaks. Additionally, when creating apps to submit to AI solutions, ensure there is sanitization within the app to remove API keys. It's important to build for a zero trust environment to avoid potential competitors or changes in learning from input information. GPT4 is an upcoming feature, and waitlists have opened for access, but it's important to also consider other models, such as Whisper, Dolly, and DaVinci.\n",
            "To remain relevant in the future, it's important to understand that even new models may not solve your problem better than previous versions. Learning Python is important for those who want to start playing with AI APIs such as Open AI, which seems to be a \"Python first environment.\" There is a Node JS API available, but it may not be at the same level as the Python API. If you know a different coding language, there may not be official APIs available, but workarounds can be found on GitHub. To start playing with GPT, it's not necessary to know much Python, and the chat GPT API initially offers 18 in credit (or possibly $5).\n",
            "If you want to start understanding AI and machine learning, sign up for the chat GPT API, which provides credit to use with text. However, using images may cost a decent amount of money. To continue using the API, simply input your credit card information. The concept of scaling silicon Dojo is to create your own martial arts studio, with no copyright on the different techniques taught. Many people have benefitted from these tutorials, with one individual from Africa downloading videos to bring back to their village without internet access.\n",
            "Silicon Dojo's founder discusses how his methods of teaching coding and programming skills can be replicated globally through affordable means. He believes that his tutorials can be followed in any location, as they only require a 700 square foot space to train others in Python and Linux programming languages. He encourages many people who have engaged with Silicon Dojo to adopt his methods in their hometowns, citing how it doesn't take many resources or financial input to make a difference. The tutorial code is open-source and available for download on GitHub to modify and use at one's discretion.\n",
            "Silicon Dojo will be focusing more on Hands-On classes to keep students engaged and awake. The plan is to cut down the three eight-hour Hands-On classes into one to three hour blocks and introduce Python as a basic concept. The focus will be on explaining modules, variables, if else statements, loops, and other important concepts. The goal is to make students comfortable with the programming language and give them a tactile experience of building something. This approach is expected to improve student engagement and help them retain more information.\n",
            "Python is the de facto language for a versatile education curriculum, covering topics such as robotics, AI, and computer vision. A proposed method involves creating 50 blocks of classes on Python, covering areas like databases, chat EPT, OpenCV, and Azure cognitive services. These blocks can be mixed and matched to cater to specific needs, such as a full-day class or boot camp. The approach mirrors a business-like method, finding a replicatable product through these Hands-On classes. Whether it will be successful, only time will tell. Overall, empowering students to create a better world is the goal of these classes, and the instructor hopes to see students attend in-person or watch on YouTube.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "83zR_WFIBa-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7efc8b7b-01f0-40b3-ee5d-465494d7b596"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/summaries/youtube/'+video_id+'.txt'\n"
      ],
      "metadata": {
        "id": "R7ZnzfXIf7kx"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path, 'w') as f:\n",
        "    f.write(final_summary)"
      ],
      "metadata": {
        "id": "SUJHN1MqgR0V"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WG7CYP4RgVhA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}