{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO609kWZ1OdFzqLAVv2D52y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chatterjeeshivani/chatgpt_experiments/blob/main/Youtube_Summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "ooEIje9r1Knd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "727273da-5c5a-42d3-c6ef-116db27e664c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.2 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "JK20WGTSp_gt",
        "outputId": "5c426ec0-7b91-4bff-e4e3-014c3e3bdfb4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a18a8e9d-516b-4b11-bb5b-b08b71a0fea9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a18a8e9d-516b-4b11-bb5b-b08b71a0fea9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving CHATGPT INTRO - Silicon Dojo Seminar.txt to CHATGPT INTRO - Silicon Dojo Seminar.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WLC3uiIVoT9O"
      },
      "outputs": [],
      "source": [
        "with open('open_api_key.txt', 'r') as f:\n",
        "    openai_key = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('CHATGPT INTRO - Silicon Dojo Seminar.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "woTszoEYp4bN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_id=\"585DQv6nmlo\""
      ],
      "metadata": {
        "id": "ufBQwecQgIxC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRkY8q1gqKcy",
        "outputId": "c5ed4687-143d-435e-e24e-af54bc7cf757"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re"
      ],
      "metadata": {
        "id": "zCjxiXiGqWDc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Divide the transcript into segments without cutting words\n",
        "def split_into_segments(text, max_segment_length=2048):\n",
        "    words = text.split()\n",
        "    segments = []\n",
        "    current_segment = []\n",
        "\n",
        "    for word in words:\n",
        "        if sum(len(w) for w in current_segment) + len(word) + len(current_segment) > max_segment_length:\n",
        "            segments.append(\" \".join(current_segment))\n",
        "            current_segment = []\n",
        "\n",
        "        current_segment.append(word)\n",
        "\n",
        "    if current_segment:\n",
        "        segments.append(\" \".join(current_segment))\n",
        "\n",
        "    return segments\n",
        "\n"
      ],
      "metadata": {
        "id": "eXgt9ZIp1Abu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segments = split_into_segments(text)\n"
      ],
      "metadata": {
        "id": "GFjW_MUB1W_q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(segments))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwlHs4Dw1ZCz",
        "outputId": "129b5823-9322-4db3-eca9-b4a27efe2b75"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(segments)"
      ],
      "metadata": {
        "id": "TMlUM1Bb1epk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key=openai_key"
      ],
      "metadata": {
        "id": "YQSdCNSHAT0f"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(segment):\n",
        "    prompt=f\"Imagine you're presenting the key points from the transcription to a group of colleagues without mentioning that it is from some other source.Try to summarise as your own points.Remove any reference to speaker or seminar. The tone should be that it is your presentation and it is a fresh presentation.Try to make it concise.Also list any important statistics, data points, or facts mentioned if any during the podcast that support the key arguments.Consider audience as technical audience so try to add technical details mentioned:\\n\\n{segment}\\n\\nSummary:\"\n",
        "    response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "          {\"role\": \"user\", \"content\": prompt}\n",
        "      ]\n",
        "    )\n",
        "    summary = response.choices[0].message.content.strip()\n",
        "    print(summary)\n",
        "    return summary\n",
        "    #print(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "YsiwJ-mR1m12"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(segments[2])\n",
        "print(generate_summary(segments[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvfI0INLAKed",
        "outputId": "aded4843-05dd-4afc-ba69-992d4b1ed6c4"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you're gonna have to remove all those disclaimers and so it might be easier to use an older model so that's the thing like when the chat kpt 4.0 comes out when you're actually able to use it one it's actually going to cost more money to use because a more powerful model so it may not matter for whatever it is that you're trying to do and also it's manner of speaking Um might be such that you realize that it's actually easier to parse the responses from older models so that's that's just something to keep in mind with this particular seminar okay so I'm going to tell you a secret I'm going to tell you a secret and to be honest this might get my geek card taken away so so take this to heart one of the ways the technology professionals like me continue to get paid a lot of money many times for doing slightly stupid tasks is Because average people people users think what we do is much more complicated than it actually is and so they go I could never do that here take a lot of money and solve my problem right this is important we start talking about things like artificial intelligence because there's a lot of people out there and they're like Eli Eli I could never do artificial intelligence I'm not good at math I don't understand statistics you know all That fancy coding I just can't get it through my head Eli I'm Gonna Leave AI to other people well here's the thing I'm gonna tell you I'm going to tell you a secret I'm not actually that great with math I did take a statistics course in college I will say I passed it I think I passed it with a c but here's the deal it doesn't actually matter right we're not really dealing with AI we're not really dealing with artificial intelligence we're Dealing with an API apis are not AI apis give us access to Ai and what this means is basically we can write 10 lines of code in Python and get all the power of AI while having no clue how we actually get the response that's the amazing thing with the modern world it's something we call serverless architecture the modern architecture\n",
            "One of the secrets to why technology professionals get paid a lot is that average people think their tasks are more complicated than they are. It's important to note that when using new technology like artificial intelligence, older models may be easier to use and cheaper. When it comes to AI, it's all about using APIs which give access to AI with very little coding required. This is made possible through serverless architecture, which is a modern architecture that allows for more power and less complexity.\n",
            "One of the secrets to why technology professionals get paid a lot is that average people think their tasks are more complicated than they are. It's important to note that when using new technology like artificial intelligence, older models may be easier to use and cheaper. When it comes to AI, it's all about using APIs which give access to AI with very little coding required. This is made possible through serverless architecture, which is a modern architecture that allows for more power and less complexity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#segments[0]"
      ],
      "metadata": {
        "id": "plgcbrs7aaxe"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = [generate_summary(segment) for segment in segments]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI9RehuZAPTz",
        "outputId": "3bb0fca6-dead-4616-d586-6541602e1df8"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today I will be presenting on the Chat GPT API, specifically how to use the DaVinci model, the 3.5 model, Dolly the image creation model, and Whisper the audio tool. While the GPT4 API just recently came out, access to it is limited to large organizations and we will focus on the current available models. These tools can be useful in creating text, images, and audio from uploaded files. To support our efforts here at Silicon Dojo, please consider donating via the link in the description.\n",
            "This seminar will be useful even if you're not teaching Chapter 4.0 because old models don't go away, and different models have different strengths. It's important to understand that when using an API, you'll be sending a request to Chatpt and parsing the text response into something the end user cares about. The Turbo 3.5 model has a lot of politically correct language that can make parsing the response more challenging, while the 3.0 model provides a cleaner response for the end user.\n",
            "In this seminar, the speaker highlights the importance of using an older model of chatbot to avoid higher costs of a more powerful model. The speaker also reveals a secret that technology professionals like him continue to get paid a lot of money for doing seemingly simple tasks because people think what they do is more complicated than it actually is. The speaker emphasizes that using AI doesn't require exceptional math or coding skills, as APIs give us access to AI with just a few lines of code in Python. The modern architecture in the tech world is built on serverless architecture.\n",
            "In this seminar, we learned about utilizing open AI to create artificial intelligence through APIs. It's a lot simpler than it might seem. All we have to do is write a few lines of code in Python, send a query to the chat GPT, receive a text response, and parse it. This is much less complicated than the actual field of artificial intelligence. As tech professionals, we often focus too much on APIs and databases while ignoring licenses, maintenance contracts, and regulations which are also important aspects of our job.\n",
            "In the United States, copyright protection is granted to humans and not to artificial intelligence (AI). If an AI creates something, nobody gets the copyright. This was illustrated by the famous \"monkey selfie\" lawsuit, in which a photographer was sued for using a photo taken by a monkey. The court ruled that since the monkey took the picture, there was no copyright. This highlights the need for legal frameworks for AI-created works.\n",
            "Recently, a court case ruled that while the script for a comic book is copyrightable, the AI-generated pictures used in the book are not. This is important to consider when creating any creative material, such as a comic book or a book. If AI is used to create graphics or characters, the creator does not own the copyright to the material. This can be a problem for startup companies who use AI to create the look and feel of their apps or products. It is important to be cautious and aware of copyright ownership when using AI-generated material.\n",
            "Copying content is a problem when people copy your work and there's no legal recourse. It's better to pay for a graphic designer to create original content that you own the copyright to. When using Dolly or Chat GPT, you still have the right to reprint, sell, and merchandise the content, but others can also copy and profit off of it. Tokens are the currency of the Chat GPT world, but they can be confusing and have varying values depending on the AI breakdown. Tokens are also very inexpensive.\n",
            "The chat GPT API uses tokens as its pricing model, which costs 1 cent for a thousand tokens. It is important to note that tokens are used for both queries and responses, and the total token cost is given when getting a response back. With the chat GPT4 model, you can give up to 8,000 coins per query, and most of the cost may come from the query rather than the response. In the enterprise world, queries are inexpensive but with heavy usage, the pricing model may become wonky. It is important to keep these pricing details in mind when using the API.\n",
            "In terms of pricing, the tokens have different models with different capabilities and price points. A thousand tokens is equivalent to about 750 words, and the cost for 1,000 tokens is one-fifth of a penny. The pricing can be confusing as each query or response is inexpensive, but it can add up with multiple queries. The Dolly model, which provides an image in response to a query, can be more expensive. It's important to be careful with the number of queries to avoid overspending. The system is still inexpensive, but for an enterprise with many users, costs can add up quickly.\n",
            "When using Dolly, different image sizes resolutions are available at various costs (1024: 2 cents per image, 512: 1.8 cents per image, 256: 1.6 cents per image). The reason for this cost is that the image generated by Dolly may not be suitable, and so multiple images will be needed. This can add up, especially if using 10 1024 images at a time, for example. Chat GPT text is relatively cheap. To use Chat GPT, set up an account and add credit card information through the billing link in the settings menu. There are monthly limits.\n",
            "When using an API like chat qpt, it's important to set a monthly cap to avoid accidentally spending hundreds or thousands of dollars due to reckless usage. Different models like GPT 3.5 turbo and DaVinci zero zero three have various capabilities and costs, with the 3.5 model being optimized for chat and significantly cheaper than DaVinci. When choosing a model, it's important to check when the training data has been updated to ensure it can accurately respond to current events or questions. The training data for these models goes up to September 2021.\n",
            "We will be comparing two language models: DaVinci and Curry Babbage and Ada model. DaVinci is better at handling language tasks with better quality, longer output, and consistent instructions. Both models have different endpoint compatibility, so there will be a need for formatting the response properly. One of the advantages of using DaVinci over the latest models is that it is less verbose, as demonstrated by the code provided.\n",
            "In order to use the Open AI chat completion function in Python, the first step is to install the module using pip. Once the module is imported, the API key needs to be called through an environment variable to prevent unauthorized access. After this, queries can be made using the chat EPT 3.5 turbo open AI chat completion create function, which sends the question to the Open AI model and returns the response. The question and response can then be printed.\n",
            "In order to use the open AI completion create, we first need to give it a prompt and a model, such as the DaVinci model. It's important to adjust variables such as temperature and max tokens to ensure the response is accurate and complete. With the DaVinci model, we use a different format for printing out the response, such as choices as zero index at text. It's important to keep in mind the token cost when asking a question, and adjust accordingly to avoid incomplete responses.\n",
            "The presentation discusses the use of different AI language models to answer subjective questions like the existence of God. Different models have different speeds, so it's important to choose the right one to parse and provide a concise and clear answer to the user. While newer models like Turbo may provide more detailed responses, they also include unnecessary information that needs to be stripped out. In contrast, the older model DaVinci provides a more succinct response which may make more sense to use. Ultimately, as technology professionals and coders, it's important to consider the user's perspective and provide an answer that they care about.\n",
            "When dealing with coding, it's important to avoid extraneous information. When parsing responses, it's important to understand the end points, which are how the response is sent back to you. When dealing with the python world, a named key array is called a dictionary, while a standard array is called a list. To print out text, you would use a response choices at index 0 message content for the chat GPT 3.5.\n",
            "When using the GPT-3 3.0 model, it is important to understand how lists and dictionaries work in order to access the value you are looking for. When printing out the text, use the format of choices > index 0 > text to get the desired response. For more complex coding needs, go through the process of finding the value you are looking for. The speaker will provide demonstrations of the DaVinci model to show its capabilities.\n",
            "In today's presentation, we will be discussing how to use DaVinci to write code and communicate with employees. The first step is to import the open AI module and add the API key. Next, we will use the open AI completion create function to send our prompt to chat GPT and receive a response. The model we will use is text DaVinci zero zero three and we will give it a prompt, a temperature, and a max token of 1,000. After this, we will print out the entire response and then just the text. Finally, our prompt will be \"tell me a story about a frog and a unicycle\".\n",
            "The DaVinci test and OpenAI's language model, python three, are crucial components of the process. The model generates stories and responses that are original and compelling. Moreover, the completion tokens were 306 with a prompt of 11 tokens that resulted in a total of 317. The generated story about a frog named Fred and his ride on a unicycle is a perfect example of the model's writing quality. It was possible to write some code using this model.\n",
            "When dealing with multiple technologies and coding languages, sometimes it is necessary to ask for help in solving problems. For example, in Django, when querying the database, the returned result is a query set, which is not a dictionary, and one may need to convert it into a dictionary. This can be achieved using the dot values method. Python can also be used for the back end of projects while JavaScript may be used for the front end, sometimes requiring assistance in triggering a python script from javascript. Asking for help and receiving code responses can vary but can be valuable in finding solutions.\n",
            "I have found a new tool that allows you to ask it questions and receive an instant answer without having to sift through ads or irrelevant content. You can ask the same question multiple times and get the same answer, but can also rephrase your question to get a different answer. This tool even allows you to format the answer in HTML which is useful for writing articles or blog posts. For example, I asked how to pour a cup of milk and received a step-by-step response with numbered lists and HTML formatting. This is a great tool for those who want to write technical blogs and articles.\n",
            "In this presentation, we will discuss the use of AI in creating automatic web pages and communication. Salesforce believes that cold calling is a low-value task and want their salespeople to focus more on high-level tasks. Chatbot integration can help create cold-call/email messages for potential clients. Using Open AI DaVinci, we can create an email with variable values to send to Sue Perkins, offering our ice cream maker at a great deal. With the rise of AI, the amount of spam is predicted to increase. Despite concerns, we can see the potential of AI in automatic web pages and communication.\n",
            "We will be discussing the 3.5 turbo model and its key features. The coating for the 3.5 model is different than the DaVinci model, but it is only five different lines of code. The turbo model allows you to assign roles to nudge the model in a particular direction, which is advantageous for reusability when creating technological products. The roles feature is useful when answering questions that depend on the user's location, as it can provide a specific answer depending on the person's location. An example of the code for the 3.5 turbo model was also shown, which included importing the open AI module, API key, and creating a variable for tweaking the answer received. No important statistics or data points were mentioned.\n",
            "In this presentation, we will discuss Chat GPT 3.5 and its three main roles: system, assistant, and user. The system role determines the character profile of the answer, such as a famous person or a three-year-old. The assistant role provides the desired direction of the answer, such as answering as a USA citizen or Brazilian. The user role is the actual question being asked. The assistant role can be used to nudge the direction of the answer and incorporate demographic information to provide the most appropriate answer. There are various messages that can be sent to Chat GPT 3.5 to refine the answer. Overall, Chat GPT 3.5 is an advanced tool for generating tailored responses.\n",
            "A dynamic system powered by open AI Turbo can give different responses based on the user's profile. The responses can be printed out with the message content, created time, model, and tokens used. Examples were given of using the system to find out who was the leader of the US and Brazil in 2000, with additional information provided for context. The response output can be a bit verbose and the speed of the system should be considered when creating web applications.\n",
            "The focus of the presentation is on the power of AI technology to customize answers for individual users based on the information collected about them. One example given is using the 3.5 turbo to concatenate values of variables to slant answers in a particular direction. The presenter emphasizes the importance of ensuring that users get the right answers by feeding additional information to the query that is being sent to chat GPT. These methods can be used to skew answers in ways that the end user may not even think about. It is noted that people mistakenly believe computers to be infallible, which makes AI technology psychologically powerful. No statistics, data points or facts were mentioned during the podcast.\n",
            "In this presentation, we will discuss the importance of considering the human condition when writing code for technology. Technology is used to solve human problems and it is important to keep in mind how the app will be used and add a little bit of humanity into it. One example of this is the use of a Da Vinci model, which is not verbose and simple in design. Additionally, we will explore how AI can be used to simplify and tailor responses based on the type of person asking questions. It is important to note that in technology, it can be tempting to rely solely on the technology to solve problems, but it is crucial to remember that technology is meant to support humans and their needs.\n",
            "The presentation talks about a list called 'slant' which includes people from different backgrounds such as Christian, scientists, pastafarians, Republicans and Democrats. The focus of the presentation is to understand the biases of the language model, chat GPT, towards different groups. The question chosen for analysis is how the world began, a topic that is often contentious with discussions on intelligent design and evolutionists. The presentation uses a for-each loop to submit the question to different groups in the slant list and then concatenates the response received from the language model. The temperature is kept at 60 to keep it simple, and the output is printed out for each group with their response to the question. The presentation raises interesting questions about biases in language models and how they can be used to skew the answer to a particular question. However, it does not provide any statistical information to support its claims.\n",
            "As a Christian, the belief is that God created the world in six days according to the Bible. As a scientist, the belief is in the Big Bang Theory which states the universe began as a single extremely hot and dense point. As a Pastafarian, the belief is that the Flying Spaghetti Monster created the world after drinking heavily. As a Republican, the belief is that God created the universe and all of its contents including the Earth and all living things. And as a Democrat, the belief is that the world began with the Big Bang Theory. It's important to consider different biases when creating an app to get the right type of answer. No statistics or data points were mentioned.\n",
            "In this presentation, we will discuss two solutions for making the blogging process easier. Firstly, we can have users or writers type in their blog post prompt, and the system will automatically concatenate and format it in HTML. This will save time and streamline the process. Secondly, we have Dolly, which is a highly advanced tool for generating images based on given prompts. However, it can be scary, so it's best suited for those who can handle it. When it comes to language, the English language differs from country to country, and we can use concatenation to make language changes according to the specific target market. This will make it more appropriate for that market. These solutions will help us to save time and produce high-quality content.\n",
            "In this presentation, we will be discussing Dolly and some important points to keep in mind when using it. One key thing to remember is that the URLs given may only be active for about an hour. If you embed an image using IMG SRC, be sure to use a tool like wget to download the image before it disappears. We will now look at some example outputs from Dolly. It can generate responses to random prompts, such as a goat on a bus going to battle clowns, or more normal prompts like a woman on a bus. Responses can range from amusing to dark depending on the input.\n",
            "In this presentation, we'll be discussing the use of OpenAI to generate images based on prompts. Some interesting prompts that have been used include a Republican in love with a sheep and a goat in World War II. OpenAI allows for the creation of multiple images with a single prompt, but this comes at a cost per image based on size. To generate images using OpenAI, one would need to import the API and call the image create function with the desired prompt and image size. The resulting images can then be embedded using HTML.\n",
            "In this presentation, we will be discussing a process to print images using IMG SRC to embed them in a single page. We will be using a Python script to process the URLs for the image and subsequently download and display them on the webpage. The webpage will be created automatically on each refresh, and we can choose from five different images to display. Additionally, the script can also process human faces, but the results may not be satisfactory.\n",
            "In this presentation, we discussed the image API with Doll E and the audio API with Whisper. Whisper is currently in beta phase and does not require tokens for transcription but this may change in the future. The Whisper API is incredibly simple, with only three lines of code needed for transcription. It also offers the ability to translate audio into 50 languages. The API key for OpenAI is required for Whisper.\n",
            "In this presentation, we will learn how to transcribe an audio file into text using a python script and the Whisper API. We will need to open the audio file using the command \"open\" and specify the file path. Then, we will use the Whisper API to transcribe the audio file into text. We will do this by using the command \"transcript = openAI_audio_transcribe_whisper_one(audio_file)\". Finally, we will print out the transcript. To do this, we need to run the python script in the command prompt. We can use a program like Audacity to record audio, and we will need to export the audio as an MP3 file. It is important to understand that dealing with audio files can be difficult, but turning an audio file into ASCII text is easy. One example of using this technology is a JavaScript code on a web page that allows you to record audio.\n",
            "A web app called Whisper can turn audio into text using MP3 format. This text can be parsed and used for queries such as weather or command prompts. The China GPT API offers moderation services for communications that can scan for negative content like hate speech or trolling. This can be useful for organizations to scan communication channels and flag red flags. As leaders and managers are not always perfect, having moderation in place can ensure the communication remains professional.\n",
            "As a manager, it is important to know which employees are struggling and getting frustrated. With tunnel vision, managers tend to focus on the best and worst performers, neglecting the middle group who are valuable but may have personal issues affecting their work. This can lead to explosions or resignations. Implementing an AI system that constantly scans messages and provides a single pane of glass to view analytics can help identify potential problems and prevent them. Every employee has a baseline, and this system can help recognize and address issues before they escalate.\n",
            "As a manager, it's important to monitor Point values for frustration and hate in employee communication. Adults still engage in bullying and intimidation, so it's important to address these issues early on to prevent lawsuits later. Implementing a system for moderation and discipline sessions can be powerful in preventing sexual harassment and discrimination lawsuits. This can be done using the import OS and open AI modules. Technical details mentioned include importing the OS and open AI modules and using input values that can come from a database or elsewhere.\n",
            "In a business environment, it is important to monitor communications for hate speech, threats of harm, sexual content, and graphic violence. The moderation API provides categories and a number score to indicate the severity of the content. If an employee expresses concerning statements, it may be necessary to intervene. The API is available in Python, and the source code shows how to use it.\n",
            "The speaker discussed the importance of utilizing text moderation to track communication trends among employees and identify deviations from the baseline. The API provides data on various types of content such as hate speech, threats, self-harm, sexual content, and violence. This data can be used to monitor and analyze communication within a company and prevent potential issues such as lawsuits. The API can be integrated with messaging systems, social media accounts, and email to track employee communication. The speaker also emphasized the significance of identifying user abuse and end users. Technical details include hate speech being 0.22um and violence being almost 1. Important facts or statistics mentioned include self-harm being 0.005 and sexual minors being 0.00.\n",
            "API violations can lead to account flags and potential loss of API access. To avoid this, it's important to be able to identify which user is violating the rules. Adding a user variable that can be traced back to a specific account can help with this. It's also important to consider the cost of using Open AI solutions like Chat GPT, as every request for an image from Dolly comes with a cost. Caching results can help minimize this cost.\n",
            "In order to save time and money, it is recommended to automatically download and store all images, even if they may not be immediately useful, as they may be useful in the future. Cache the results from GPT to provide faster responses to frequently asked questions. Humans are all relatively similar, so it is suggested to cache the questions and answers being asked and edit them to be appropriate for your particular environment. This will provide more accurate and efficient responses to users and reduce costs associated with API fees.\n",
            "We need to be mindful of API usage fees as they can become costly with high web app usage. Improving operational security is important by utilizing caching, which keeps communications local and reduces risks of man in the middle attacks. It is also important to note that currently, chat EPT is not using questions to teach their models. However, it is essential to consider that self-learning models, such as chat EPT, have the potential to learn and replicate proprietary code. This was an issue with Amazon engineers copying blocks of proprietary code into chat EPT, which could cost the company millions of dollars if learned by others.\n",
            "When using APIs, it is important to be careful as vendors may log information and users may not know what queries are being used. It is important to sanitize information and only send what is necessary. Developers should also ensure there is sanitization within the app to remove API keys to prevent leaks. It is important to build for a zero trust environment to avoid security issues. The seminar did not cover GPT4, but a live demo will be available through the API waitlist.\n",
            "The speaker emphasizes the importance of remaining relevant in the future, using DaVinci AI to solve problems. Even with the release of the new version, 4, it doesn't discount the benefits of using DaVinci. If you want to start playing with the AI API, make sure to learn Python as it appears to be the go-to language for OpenAI. Although there is a NodeJS API, it's not at the same level as the Python API. If you know a different coding language, there may be ways to work around the system, but Python is the recommended language. Additionally, the speaker shows how to use the Chat GPT API and suggests signing up for it to get a credit.\n",
            "In this presentation, we learned about the benefits of signing up for the GPT API, which provides a certain amount of credit to use with AI. It can be cost-effective, but working with images can use up more credit. The concept of Silicon Dojo is about scaling and empowering others to create their own success, similar to martial arts, where one can reach a level and then teach it to others. The presenter encourages us to steal this presentation and use it to help with our own work.\n",
            "Silicon Dojo has the potential to scale and grow by having people in different locations make their own Dojo using the information provided. The presenter currently has a 700 square foot room in Asheville, North Carolina, but the Dojo can be created in a basement or a garage with minimal resources. The education provided can be modified to suit the local environment and empower individuals in their area. The seminar was enjoyable to teach virtually and the code will be available on GitHub for download and modification. No specific statistics, data points or facts were mentioned during the podcast.\n",
            "The speaker is planning to shift the focus towards more Hands-On classes rather than seminars. This is because students tend to fall asleep during seminars after a long day of work. The idea is to make it more interactive and engaging for students through exercises and Hands-On experience. They plan to offer shorter classes, introducing topics such as Python, before moving onto more advanced topics such as chat GPT or Dolly. The speaker believes this modular approach will provide students with a better understanding and allow for a more engaging experience.\n",
            "We can solve problems in education by creating 50 classes that can be picked and chosen for specific students. Python will be our go-to language because of its versatility in building robots, dealing with AI, and computer vision. We can have one block class every Tuesday or on weekends for a day, consisting of two or three blocks combined on different subjects like databases, chat EPT, opencv, and Azure cognitive services. We can also create a boot camp for five days to give a basic understanding of python, Django, databases, and connect it with machine vision, etc. Creating blocks of classes will help in finding a replicatable product that aligns with business principles. The success of this plan will depend on the outcome in the next few months. The presenter's goal is to empower people and make the world a better place by teaching and conducting hands-on classes. There will be in-person classes and videos on YouTube to watch. Technical details: Python is versatile in building robots, dealing with AI, and computer vision.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#summaries"
      ],
      "metadata": {
        "id": "GayYP6coDEuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Combine the summaries to create a step-by-step summary of the podcast\n",
        "\n",
        "final_summary = \"\\n\".join(summary for summary in summaries if summary is not None)\n",
        "print(\"Step-by-step summary of the podcast:\")\n",
        "print(final_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2F56lnfA1Qx",
        "outputId": "db8dd392-deba-4a2e-9e59-42914e466f4b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-by-step summary of the podcast:\n",
            "Today I will be presenting on the Chat GPT API, specifically how to use the DaVinci model, the 3.5 model, Dolly the image creation model, and Whisper the audio tool. While the GPT4 API just recently came out, access to it is limited to large organizations and we will focus on the current available models. These tools can be useful in creating text, images, and audio from uploaded files. To support our efforts here at Silicon Dojo, please consider donating via the link in the description.\n",
            "This seminar will be useful even if you're not teaching Chapter 4.0 because old models don't go away, and different models have different strengths. It's important to understand that when using an API, you'll be sending a request to Chatpt and parsing the text response into something the end user cares about. The Turbo 3.5 model has a lot of politically correct language that can make parsing the response more challenging, while the 3.0 model provides a cleaner response for the end user.\n",
            "In this seminar, the speaker highlights the importance of using an older model of chatbot to avoid higher costs of a more powerful model. The speaker also reveals a secret that technology professionals like him continue to get paid a lot of money for doing seemingly simple tasks because people think what they do is more complicated than it actually is. The speaker emphasizes that using AI doesn't require exceptional math or coding skills, as APIs give us access to AI with just a few lines of code in Python. The modern architecture in the tech world is built on serverless architecture.\n",
            "In this seminar, we learned about utilizing open AI to create artificial intelligence through APIs. It's a lot simpler than it might seem. All we have to do is write a few lines of code in Python, send a query to the chat GPT, receive a text response, and parse it. This is much less complicated than the actual field of artificial intelligence. As tech professionals, we often focus too much on APIs and databases while ignoring licenses, maintenance contracts, and regulations which are also important aspects of our job.\n",
            "In the United States, copyright protection is granted to humans and not to artificial intelligence (AI). If an AI creates something, nobody gets the copyright. This was illustrated by the famous \"monkey selfie\" lawsuit, in which a photographer was sued for using a photo taken by a monkey. The court ruled that since the monkey took the picture, there was no copyright. This highlights the need for legal frameworks for AI-created works.\n",
            "Recently, a court case ruled that while the script for a comic book is copyrightable, the AI-generated pictures used in the book are not. This is important to consider when creating any creative material, such as a comic book or a book. If AI is used to create graphics or characters, the creator does not own the copyright to the material. This can be a problem for startup companies who use AI to create the look and feel of their apps or products. It is important to be cautious and aware of copyright ownership when using AI-generated material.\n",
            "Copying content is a problem when people copy your work and there's no legal recourse. It's better to pay for a graphic designer to create original content that you own the copyright to. When using Dolly or Chat GPT, you still have the right to reprint, sell, and merchandise the content, but others can also copy and profit off of it. Tokens are the currency of the Chat GPT world, but they can be confusing and have varying values depending on the AI breakdown. Tokens are also very inexpensive.\n",
            "The chat GPT API uses tokens as its pricing model, which costs 1 cent for a thousand tokens. It is important to note that tokens are used for both queries and responses, and the total token cost is given when getting a response back. With the chat GPT4 model, you can give up to 8,000 coins per query, and most of the cost may come from the query rather than the response. In the enterprise world, queries are inexpensive but with heavy usage, the pricing model may become wonky. It is important to keep these pricing details in mind when using the API.\n",
            "In terms of pricing, the tokens have different models with different capabilities and price points. A thousand tokens is equivalent to about 750 words, and the cost for 1,000 tokens is one-fifth of a penny. The pricing can be confusing as each query or response is inexpensive, but it can add up with multiple queries. The Dolly model, which provides an image in response to a query, can be more expensive. It's important to be careful with the number of queries to avoid overspending. The system is still inexpensive, but for an enterprise with many users, costs can add up quickly.\n",
            "When using Dolly, different image sizes resolutions are available at various costs (1024: 2 cents per image, 512: 1.8 cents per image, 256: 1.6 cents per image). The reason for this cost is that the image generated by Dolly may not be suitable, and so multiple images will be needed. This can add up, especially if using 10 1024 images at a time, for example. Chat GPT text is relatively cheap. To use Chat GPT, set up an account and add credit card information through the billing link in the settings menu. There are monthly limits.\n",
            "When using an API like chat qpt, it's important to set a monthly cap to avoid accidentally spending hundreds or thousands of dollars due to reckless usage. Different models like GPT 3.5 turbo and DaVinci zero zero three have various capabilities and costs, with the 3.5 model being optimized for chat and significantly cheaper than DaVinci. When choosing a model, it's important to check when the training data has been updated to ensure it can accurately respond to current events or questions. The training data for these models goes up to September 2021.\n",
            "We will be comparing two language models: DaVinci and Curry Babbage and Ada model. DaVinci is better at handling language tasks with better quality, longer output, and consistent instructions. Both models have different endpoint compatibility, so there will be a need for formatting the response properly. One of the advantages of using DaVinci over the latest models is that it is less verbose, as demonstrated by the code provided.\n",
            "In order to use the Open AI chat completion function in Python, the first step is to install the module using pip. Once the module is imported, the API key needs to be called through an environment variable to prevent unauthorized access. After this, queries can be made using the chat EPT 3.5 turbo open AI chat completion create function, which sends the question to the Open AI model and returns the response. The question and response can then be printed.\n",
            "In order to use the open AI completion create, we first need to give it a prompt and a model, such as the DaVinci model. It's important to adjust variables such as temperature and max tokens to ensure the response is accurate and complete. With the DaVinci model, we use a different format for printing out the response, such as choices as zero index at text. It's important to keep in mind the token cost when asking a question, and adjust accordingly to avoid incomplete responses.\n",
            "The presentation discusses the use of different AI language models to answer subjective questions like the existence of God. Different models have different speeds, so it's important to choose the right one to parse and provide a concise and clear answer to the user. While newer models like Turbo may provide more detailed responses, they also include unnecessary information that needs to be stripped out. In contrast, the older model DaVinci provides a more succinct response which may make more sense to use. Ultimately, as technology professionals and coders, it's important to consider the user's perspective and provide an answer that they care about.\n",
            "When dealing with coding, it's important to avoid extraneous information. When parsing responses, it's important to understand the end points, which are how the response is sent back to you. When dealing with the python world, a named key array is called a dictionary, while a standard array is called a list. To print out text, you would use a response choices at index 0 message content for the chat GPT 3.5.\n",
            "When using the GPT-3 3.0 model, it is important to understand how lists and dictionaries work in order to access the value you are looking for. When printing out the text, use the format of choices > index 0 > text to get the desired response. For more complex coding needs, go through the process of finding the value you are looking for. The speaker will provide demonstrations of the DaVinci model to show its capabilities.\n",
            "In today's presentation, we will be discussing how to use DaVinci to write code and communicate with employees. The first step is to import the open AI module and add the API key. Next, we will use the open AI completion create function to send our prompt to chat GPT and receive a response. The model we will use is text DaVinci zero zero three and we will give it a prompt, a temperature, and a max token of 1,000. After this, we will print out the entire response and then just the text. Finally, our prompt will be \"tell me a story about a frog and a unicycle\".\n",
            "The DaVinci test and OpenAI's language model, python three, are crucial components of the process. The model generates stories and responses that are original and compelling. Moreover, the completion tokens were 306 with a prompt of 11 tokens that resulted in a total of 317. The generated story about a frog named Fred and his ride on a unicycle is a perfect example of the model's writing quality. It was possible to write some code using this model.\n",
            "When dealing with multiple technologies and coding languages, sometimes it is necessary to ask for help in solving problems. For example, in Django, when querying the database, the returned result is a query set, which is not a dictionary, and one may need to convert it into a dictionary. This can be achieved using the dot values method. Python can also be used for the back end of projects while JavaScript may be used for the front end, sometimes requiring assistance in triggering a python script from javascript. Asking for help and receiving code responses can vary but can be valuable in finding solutions.\n",
            "I have found a new tool that allows you to ask it questions and receive an instant answer without having to sift through ads or irrelevant content. You can ask the same question multiple times and get the same answer, but can also rephrase your question to get a different answer. This tool even allows you to format the answer in HTML which is useful for writing articles or blog posts. For example, I asked how to pour a cup of milk and received a step-by-step response with numbered lists and HTML formatting. This is a great tool for those who want to write technical blogs and articles.\n",
            "In this presentation, we will discuss the use of AI in creating automatic web pages and communication. Salesforce believes that cold calling is a low-value task and want their salespeople to focus more on high-level tasks. Chatbot integration can help create cold-call/email messages for potential clients. Using Open AI DaVinci, we can create an email with variable values to send to Sue Perkins, offering our ice cream maker at a great deal. With the rise of AI, the amount of spam is predicted to increase. Despite concerns, we can see the potential of AI in automatic web pages and communication.\n",
            "We will be discussing the 3.5 turbo model and its key features. The coating for the 3.5 model is different than the DaVinci model, but it is only five different lines of code. The turbo model allows you to assign roles to nudge the model in a particular direction, which is advantageous for reusability when creating technological products. The roles feature is useful when answering questions that depend on the user's location, as it can provide a specific answer depending on the person's location. An example of the code for the 3.5 turbo model was also shown, which included importing the open AI module, API key, and creating a variable for tweaking the answer received. No important statistics or data points were mentioned.\n",
            "In this presentation, we will discuss Chat GPT 3.5 and its three main roles: system, assistant, and user. The system role determines the character profile of the answer, such as a famous person or a three-year-old. The assistant role provides the desired direction of the answer, such as answering as a USA citizen or Brazilian. The user role is the actual question being asked. The assistant role can be used to nudge the direction of the answer and incorporate demographic information to provide the most appropriate answer. There are various messages that can be sent to Chat GPT 3.5 to refine the answer. Overall, Chat GPT 3.5 is an advanced tool for generating tailored responses.\n",
            "A dynamic system powered by open AI Turbo can give different responses based on the user's profile. The responses can be printed out with the message content, created time, model, and tokens used. Examples were given of using the system to find out who was the leader of the US and Brazil in 2000, with additional information provided for context. The response output can be a bit verbose and the speed of the system should be considered when creating web applications.\n",
            "The focus of the presentation is on the power of AI technology to customize answers for individual users based on the information collected about them. One example given is using the 3.5 turbo to concatenate values of variables to slant answers in a particular direction. The presenter emphasizes the importance of ensuring that users get the right answers by feeding additional information to the query that is being sent to chat GPT. These methods can be used to skew answers in ways that the end user may not even think about. It is noted that people mistakenly believe computers to be infallible, which makes AI technology psychologically powerful. No statistics, data points or facts were mentioned during the podcast.\n",
            "In this presentation, we will discuss the importance of considering the human condition when writing code for technology. Technology is used to solve human problems and it is important to keep in mind how the app will be used and add a little bit of humanity into it. One example of this is the use of a Da Vinci model, which is not verbose and simple in design. Additionally, we will explore how AI can be used to simplify and tailor responses based on the type of person asking questions. It is important to note that in technology, it can be tempting to rely solely on the technology to solve problems, but it is crucial to remember that technology is meant to support humans and their needs.\n",
            "The presentation talks about a list called 'slant' which includes people from different backgrounds such as Christian, scientists, pastafarians, Republicans and Democrats. The focus of the presentation is to understand the biases of the language model, chat GPT, towards different groups. The question chosen for analysis is how the world began, a topic that is often contentious with discussions on intelligent design and evolutionists. The presentation uses a for-each loop to submit the question to different groups in the slant list and then concatenates the response received from the language model. The temperature is kept at 60 to keep it simple, and the output is printed out for each group with their response to the question. The presentation raises interesting questions about biases in language models and how they can be used to skew the answer to a particular question. However, it does not provide any statistical information to support its claims.\n",
            "As a Christian, the belief is that God created the world in six days according to the Bible. As a scientist, the belief is in the Big Bang Theory which states the universe began as a single extremely hot and dense point. As a Pastafarian, the belief is that the Flying Spaghetti Monster created the world after drinking heavily. As a Republican, the belief is that God created the universe and all of its contents including the Earth and all living things. And as a Democrat, the belief is that the world began with the Big Bang Theory. It's important to consider different biases when creating an app to get the right type of answer. No statistics or data points were mentioned.\n",
            "In this presentation, we will discuss two solutions for making the blogging process easier. Firstly, we can have users or writers type in their blog post prompt, and the system will automatically concatenate and format it in HTML. This will save time and streamline the process. Secondly, we have Dolly, which is a highly advanced tool for generating images based on given prompts. However, it can be scary, so it's best suited for those who can handle it. When it comes to language, the English language differs from country to country, and we can use concatenation to make language changes according to the specific target market. This will make it more appropriate for that market. These solutions will help us to save time and produce high-quality content.\n",
            "In this presentation, we will be discussing Dolly and some important points to keep in mind when using it. One key thing to remember is that the URLs given may only be active for about an hour. If you embed an image using IMG SRC, be sure to use a tool like wget to download the image before it disappears. We will now look at some example outputs from Dolly. It can generate responses to random prompts, such as a goat on a bus going to battle clowns, or more normal prompts like a woman on a bus. Responses can range from amusing to dark depending on the input.\n",
            "In this presentation, we'll be discussing the use of OpenAI to generate images based on prompts. Some interesting prompts that have been used include a Republican in love with a sheep and a goat in World War II. OpenAI allows for the creation of multiple images with a single prompt, but this comes at a cost per image based on size. To generate images using OpenAI, one would need to import the API and call the image create function with the desired prompt and image size. The resulting images can then be embedded using HTML.\n",
            "In this presentation, we will be discussing a process to print images using IMG SRC to embed them in a single page. We will be using a Python script to process the URLs for the image and subsequently download and display them on the webpage. The webpage will be created automatically on each refresh, and we can choose from five different images to display. Additionally, the script can also process human faces, but the results may not be satisfactory.\n",
            "In this presentation, we discussed the image API with Doll E and the audio API with Whisper. Whisper is currently in beta phase and does not require tokens for transcription but this may change in the future. The Whisper API is incredibly simple, with only three lines of code needed for transcription. It also offers the ability to translate audio into 50 languages. The API key for OpenAI is required for Whisper.\n",
            "In this presentation, we will learn how to transcribe an audio file into text using a python script and the Whisper API. We will need to open the audio file using the command \"open\" and specify the file path. Then, we will use the Whisper API to transcribe the audio file into text. We will do this by using the command \"transcript = openAI_audio_transcribe_whisper_one(audio_file)\". Finally, we will print out the transcript. To do this, we need to run the python script in the command prompt. We can use a program like Audacity to record audio, and we will need to export the audio as an MP3 file. It is important to understand that dealing with audio files can be difficult, but turning an audio file into ASCII text is easy. One example of using this technology is a JavaScript code on a web page that allows you to record audio.\n",
            "A web app called Whisper can turn audio into text using MP3 format. This text can be parsed and used for queries such as weather or command prompts. The China GPT API offers moderation services for communications that can scan for negative content like hate speech or trolling. This can be useful for organizations to scan communication channels and flag red flags. As leaders and managers are not always perfect, having moderation in place can ensure the communication remains professional.\n",
            "As a manager, it is important to know which employees are struggling and getting frustrated. With tunnel vision, managers tend to focus on the best and worst performers, neglecting the middle group who are valuable but may have personal issues affecting their work. This can lead to explosions or resignations. Implementing an AI system that constantly scans messages and provides a single pane of glass to view analytics can help identify potential problems and prevent them. Every employee has a baseline, and this system can help recognize and address issues before they escalate.\n",
            "As a manager, it's important to monitor Point values for frustration and hate in employee communication. Adults still engage in bullying and intimidation, so it's important to address these issues early on to prevent lawsuits later. Implementing a system for moderation and discipline sessions can be powerful in preventing sexual harassment and discrimination lawsuits. This can be done using the import OS and open AI modules. Technical details mentioned include importing the OS and open AI modules and using input values that can come from a database or elsewhere.\n",
            "In a business environment, it is important to monitor communications for hate speech, threats of harm, sexual content, and graphic violence. The moderation API provides categories and a number score to indicate the severity of the content. If an employee expresses concerning statements, it may be necessary to intervene. The API is available in Python, and the source code shows how to use it.\n",
            "The speaker discussed the importance of utilizing text moderation to track communication trends among employees and identify deviations from the baseline. The API provides data on various types of content such as hate speech, threats, self-harm, sexual content, and violence. This data can be used to monitor and analyze communication within a company and prevent potential issues such as lawsuits. The API can be integrated with messaging systems, social media accounts, and email to track employee communication. The speaker also emphasized the significance of identifying user abuse and end users. Technical details include hate speech being 0.22um and violence being almost 1. Important facts or statistics mentioned include self-harm being 0.005 and sexual minors being 0.00.\n",
            "API violations can lead to account flags and potential loss of API access. To avoid this, it's important to be able to identify which user is violating the rules. Adding a user variable that can be traced back to a specific account can help with this. It's also important to consider the cost of using Open AI solutions like Chat GPT, as every request for an image from Dolly comes with a cost. Caching results can help minimize this cost.\n",
            "In order to save time and money, it is recommended to automatically download and store all images, even if they may not be immediately useful, as they may be useful in the future. Cache the results from GPT to provide faster responses to frequently asked questions. Humans are all relatively similar, so it is suggested to cache the questions and answers being asked and edit them to be appropriate for your particular environment. This will provide more accurate and efficient responses to users and reduce costs associated with API fees.\n",
            "We need to be mindful of API usage fees as they can become costly with high web app usage. Improving operational security is important by utilizing caching, which keeps communications local and reduces risks of man in the middle attacks. It is also important to note that currently, chat EPT is not using questions to teach their models. However, it is essential to consider that self-learning models, such as chat EPT, have the potential to learn and replicate proprietary code. This was an issue with Amazon engineers copying blocks of proprietary code into chat EPT, which could cost the company millions of dollars if learned by others.\n",
            "When using APIs, it is important to be careful as vendors may log information and users may not know what queries are being used. It is important to sanitize information and only send what is necessary. Developers should also ensure there is sanitization within the app to remove API keys to prevent leaks. It is important to build for a zero trust environment to avoid security issues. The seminar did not cover GPT4, but a live demo will be available through the API waitlist.\n",
            "The speaker emphasizes the importance of remaining relevant in the future, using DaVinci AI to solve problems. Even with the release of the new version, 4, it doesn't discount the benefits of using DaVinci. If you want to start playing with the AI API, make sure to learn Python as it appears to be the go-to language for OpenAI. Although there is a NodeJS API, it's not at the same level as the Python API. If you know a different coding language, there may be ways to work around the system, but Python is the recommended language. Additionally, the speaker shows how to use the Chat GPT API and suggests signing up for it to get a credit.\n",
            "In this presentation, we learned about the benefits of signing up for the GPT API, which provides a certain amount of credit to use with AI. It can be cost-effective, but working with images can use up more credit. The concept of Silicon Dojo is about scaling and empowering others to create their own success, similar to martial arts, where one can reach a level and then teach it to others. The presenter encourages us to steal this presentation and use it to help with our own work.\n",
            "Silicon Dojo has the potential to scale and grow by having people in different locations make their own Dojo using the information provided. The presenter currently has a 700 square foot room in Asheville, North Carolina, but the Dojo can be created in a basement or a garage with minimal resources. The education provided can be modified to suit the local environment and empower individuals in their area. The seminar was enjoyable to teach virtually and the code will be available on GitHub for download and modification. No specific statistics, data points or facts were mentioned during the podcast.\n",
            "The speaker is planning to shift the focus towards more Hands-On classes rather than seminars. This is because students tend to fall asleep during seminars after a long day of work. The idea is to make it more interactive and engaging for students through exercises and Hands-On experience. They plan to offer shorter classes, introducing topics such as Python, before moving onto more advanced topics such as chat GPT or Dolly. The speaker believes this modular approach will provide students with a better understanding and allow for a more engaging experience.\n",
            "We can solve problems in education by creating 50 classes that can be picked and chosen for specific students. Python will be our go-to language because of its versatility in building robots, dealing with AI, and computer vision. We can have one block class every Tuesday or on weekends for a day, consisting of two or three blocks combined on different subjects like databases, chat EPT, opencv, and Azure cognitive services. We can also create a boot camp for five days to give a basic understanding of python, Django, databases, and connect it with machine vision, etc. Creating blocks of classes will help in finding a replicatable product that aligns with business principles. The success of this plan will depend on the outcome in the next few months. The presenter's goal is to empower people and make the world a better place by teaching and conducting hands-on classes. There will be in-person classes and videos on YouTube to watch. Technical details: Python is versatile in building robots, dealing with AI, and computer vision.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "83zR_WFIBa-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0314ba06-e53e-44ed-c3d1-013cfbff8741"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/summaries/youtube/'+video_id+'.txt'\n"
      ],
      "metadata": {
        "id": "R7ZnzfXIf7kx"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path, 'w') as f:\n",
        "    f.write(final_summary)"
      ],
      "metadata": {
        "id": "SUJHN1MqgR0V"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WG7CYP4RgVhA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}